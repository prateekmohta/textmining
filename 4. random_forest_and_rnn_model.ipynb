{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import spacy \n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, Flatten, MaxPooling1D, AveragePooling1D, Reshape, SimpleRNN, \\\n",
    "TimeDistributed, Bidirectional, BatchNormalization\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler(Callback):\n",
    "\n",
    "    def __init__(self, factor=0.6, loss_trigger=0.11, min_lr=1e-5):\n",
    "        super(LRScheduler, self).__init__()\n",
    "\n",
    "        self.factor = factor\n",
    "        self.loss_trigger = loss_trigger\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = np.Inf\n",
    "        \n",
    "    def _reduce_lr(self):\n",
    "        old_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "        if old_lr > self.min_lr:\n",
    "            new_lr = old_lr * self.factor\n",
    "            new_lr = max(new_lr, self.min_lr)\n",
    "            K.set_value(self.model.optimizer.lr, new_lr)\n",
    "            print(' $ LRScheduler reducing learning rate to %s.' % (new_lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(\"val_loss\")\n",
    "        \n",
    "        if not np.less(current, self.best) or np.less(current, self.loss_trigger):\n",
    "            self._reduce_lr()\n",
    "        \n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def load_glove_embeddings(fp, embedding_dim, include_empty_char=True):\n",
    "    \"\"\"\n",
    "    Loads pre-trained word embeddings (GloVe embeddings)\n",
    "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
    "                - embedding_dim: dimension of each vector embedding\n",
    "                - generate_matrix: whether to generate an embedding matrix\n",
    "        Outputs:\n",
    "                - word2coefs: Dictionary. Word to its corresponding coefficients\n",
    "                - word2index: Dictionary. Word to word-index\n",
    "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
    "    \"\"\"\n",
    "    # First, build the \"word2coefs\" and \"word2index\"\n",
    "    word2coefs = {} # word to its corresponding coefficients\n",
    "    word2index = {} # word to word-index\n",
    "    with open(fp) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                data = [x.strip().lower() for x in line.split()]\n",
    "                word = data[0]\n",
    "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
    "                word2coefs[word] = coefs\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "            except Exception as e:\n",
    "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
    "                continue\n",
    "        # End of for loop.\n",
    "    # End of with open\n",
    "    if include_empty_char:\n",
    "        word2index[''] = len(word2index)\n",
    "    # Second, build the \"embedding_matrix\"\n",
    "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
    "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word2index.items():\n",
    "        embedding_vec = word2coefs.get(word)\n",
    "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
    "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
    "    # return word2coefs, word2index, embedding_matrix\n",
    "    return word2index, np.asarray(embedding_matrix)\n",
    "\n",
    "def custom_tokenize(docs):\n",
    "    output_matrix = []\n",
    "    for d in docs:\n",
    "        indices = []\n",
    "        for w in d.split():\n",
    "            try:\n",
    "                indices.append(word2index[re.sub(r'[^\\w\\s]','',w).lower()])\n",
    "            except:\n",
    "                pass\n",
    "        output_matrix.append(indices)\n",
    "    return output_matrix\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"reviews.csv\")\n",
    "data=data[['Review Text','Rating']]\n",
    "data = data[data['Review Text'].isnull()==False]\n",
    "data = data.reset_index()\n",
    "Eindex = list(filter(lambda x:isEnglish(data['Review Text'][x]),range(data.shape[0])))\n",
    "data = data.iloc[Eindex,:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Review Text'], data['Rating'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    231919\n",
       "4     46797\n",
       "1     27369\n",
       "3     23279\n",
       "2      9293\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, embedding_matrix = load_glove_embeddings('glove.6B.200d.txt', embedding_dim=200)\n",
    "X_train_oh = custom_tokenize(X_train)\n",
    "X_test_oh = custom_tokenize(X_test)\n",
    "maxlen = 40\n",
    "X_train_oh = pad_sequences(X_train_oh, maxlen=maxlen, padding='post')\n",
    "X_test_oh = pad_sequences(X_test_oh, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Bag of words\" random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words = 'english',min_df=20,token_pattern='[a-z]+')\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train_dtm, y_train)\n",
    "y_pred_class = rf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.948403134168\n",
      "AMPE:  25.8583564754\n"
     ]
    }
   ],
   "source": [
    "print('RMSE: ', metrics.mean_squared_error(y_test,y_pred_class))\n",
    "print('AMPE: ', mean_absolute_percentage_error(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1], \n",
    "                            input_length=maxlen,\n",
    "                            weights=[embedding_matrix], \n",
    "                            trainable=False, \n",
    "                            name='embedding_layer'))\n",
    "model1.add(LSTM(128, dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(1, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 253992 samples, validate on 84665 samples\n",
      "Epoch 1/10\n",
      "253992/253992 [==============================] - 138s 545us/step - loss: 0.8159 - mean_absolute_percentage_error: 26.1224 - val_loss: 0.7774 - val_mean_absolute_percentage_error: 24.6438\n",
      "Epoch 2/10\n",
      "253992/253992 [==============================] - 138s 543us/step - loss: 0.7963 - mean_absolute_percentage_error: 25.6860 - val_loss: 0.7719 - val_mean_absolute_percentage_error: 24.5353\n",
      "Epoch 3/10\n",
      "253800/253992 [============================>.] - ETA: 0s - loss: 0.7912 - mean_absolute_percentage_error: 25.5043 $ LRScheduler reducing learning rate to 0.0007000000332482159.\n",
      "253992/253992 [==============================] - 138s 542us/step - loss: 0.7911 - mean_absolute_percentage_error: 25.5008 - val_loss: 0.7742 - val_mean_absolute_percentage_error: 25.5931\n",
      "Epoch 4/10\n",
      "253992/253992 [==============================] - 137s 541us/step - loss: 0.7871 - mean_absolute_percentage_error: 25.4506 - val_loss: 0.7692 - val_mean_absolute_percentage_error: 24.6278\n",
      "Epoch 5/10\n",
      "253800/253992 [============================>.] - ETA: 0s - loss: 0.7865 - mean_absolute_percentage_error: 25.4236 $ LRScheduler reducing learning rate to 0.0004900000232737511.\n",
      "253992/253992 [==============================] - 137s 539us/step - loss: 0.7864 - mean_absolute_percentage_error: 25.4207 - val_loss: 0.7693 - val_mean_absolute_percentage_error: 24.6482\n",
      "Epoch 6/10\n",
      "253992/253992 [==============================] - 137s 538us/step - loss: 0.7869 - mean_absolute_percentage_error: 25.4519 - val_loss: 0.7691 - val_mean_absolute_percentage_error: 24.6370\n",
      "Epoch 7/10\n",
      "253992/253992 [==============================] - 136s 537us/step - loss: 0.7824 - mean_absolute_percentage_error: 25.3318 - val_loss: 0.7674 - val_mean_absolute_percentage_error: 24.7290\n",
      "Epoch 8/10\n",
      "253992/253992 [==============================] - 136s 537us/step - loss: 0.7855 - mean_absolute_percentage_error: 25.3868 - val_loss: 0.7670 - val_mean_absolute_percentage_error: 24.7975\n",
      "Epoch 9/10\n",
      "253992/253992 [==============================] - 136s 537us/step - loss: 0.7861 - mean_absolute_percentage_error: 25.4255 - val_loss: 0.7663 - val_mean_absolute_percentage_error: 24.6173\n",
      "Epoch 10/10\n",
      "253800/253992 [============================>.] - ETA: 0s - loss: 0.7835 - mean_absolute_percentage_error: 25.3446 $ LRScheduler reducing learning rate to 0.00034300000406801696.\n",
      "253992/253992 [==============================] - 136s 537us/step - loss: 0.7836 - mean_absolute_percentage_error: 25.3441 - val_loss: 0.7666 - val_mean_absolute_percentage_error: 24.5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fd02eaf98>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=10, verbose=1)\n",
    "reduce_lr = LRScheduler(factor=0.7, min_lr=5e-5)\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,decay=0.01)\n",
    "model1.compile(loss='mean_squared_error', optimizer=adam, metrics=['mape'])\n",
    "model1.fit(X_train_oh, y_train, batch_size=200, epochs=50,validation_data = (X_test_oh,y_test),callbacks=[early_stop,reduce_lr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
